{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4469114c",
   "metadata": {},
   "source": [
    "1. MLP의 구현 조건은 다음과 같습니다.\n",
    "\n",
    "   - 5층(입력층, 은닉층 3개, 출력층)을 가지는 분류용 MLP를 구현합니다.\n",
    "\n",
    "   - 각 은닉층의 노드 수는 입력데이터에 따라 적절히 설정합니다.\n",
    "\n",
    "   - 입력은 미니배치로 배치크기를 다르게 입력할 수 있도록 합니다.\n",
    "\n",
    "   - 출력은 MNIST와 같이  10가지 경우를 구별할 수 있게 만듭니다.\n",
    "\n",
    "   - 클래스를 적절히 이용해 구현합니다.\n",
    "\n",
    "\n",
    "\n",
    "2. MLP 학습코드 구현 조건은 다음과 같습니다.\n",
    "\n",
    "   - 오류역전파 방식으로 학습을 수행합니다.\n",
    "\n",
    "   - 최적화 알고리즘으로 SGD, Adam을 선택해 학습 가능하도록 합니다.\n",
    "\n",
    "   - 과적합 방지 알고리즘으로 Batch Normalization, Weight Decay을 선택해 학습 가능하도록 합니다.\n",
    "\n",
    "   - 클래스를 적절히 이용해 구현합니다.\n",
    "\n",
    "\n",
    "\n",
    "3. 학습 데이터 \n",
    "\n",
    "   - fashion-MNIST를 사용합니다.\n",
    "\n",
    "   - fashion-MNIST 데이터는 https://github.com/zalandoresearch/fashion-mnist 에서 다운받아 사용합니다.\n",
    "\n",
    "   - 학습데이터를 이용해 학습하고 시험데이터를 이용해 평가합니다.\n",
    "\n",
    "\n",
    "\n",
    "4. 학습 결과 출력: 다음 결과를 도출합니다.\n",
    "\n",
    "   - 데이터셋: Fashion MNIST\n",
    "\n",
    "   - 미니배치 사이즈: 10, 100, 1000\n",
    "\n",
    "   - 에폭 수: 30\n",
    "\n",
    "   - 최적화 알고리즘: SGD, Adam \n",
    "\n",
    "   - 과적합 방지 알고리즘: 사용X, Batch Normalization, Weight Decay\n",
    "\n",
    "   - 18개 설정(미니배치3X최적화2X과적합방지3)에 대해 모두 학습하고 학습 정확도 및 시험 정확도를 수치와 그래프로 표시하되 시험정확도가 높은 순으로 표시하세요.\n",
    "\n",
    "   - 추가로, 가장 시험정확도가 높은 모델을 이용해 시험데이터 중 샘플로 5개를 읽어들여 그림을 표시하고 분류결과를 표시하여 확인하는 코드를 작성하세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e98aeeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5370b259",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = mnist_reader.load_mnist('../data/fashion', kind='train')\n",
    "X_test, y_test = mnist_reader.load_mnist(\"../data/fashion\", kind=\"t10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6deae960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30af56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_model import SGD, Adam,ThreeLayerNet, Sigmoid, Affine, SoftmaxWithLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c036f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df1eb89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = ThreeLayerNet(input_size =784, hidden_size = 50, output_size = 10)\n",
    "optimizer = Adam()\n",
    "iters_num = 10000\n",
    "train_size = X_train.shape[0]\n",
    "test_size = X_test.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = {}\n",
    "train_acc_list = {}\n",
    "test_acc_list = {}\n",
    "\n",
    "train_loss_list[\"SGD\"] = []\n",
    "train_acc_list[\"SGD\"] = []\n",
    "test_acc_list[\"SGD\"] = []\n",
    "\n",
    "\n",
    "train_loss_list[\"Adam\"] = []\n",
    "train_acc_list[\"Adam\"] = []\n",
    "test_acc_list[\"Adam\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87bff4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 : train acc = 0.75 , test acc = 0.7815\n",
      "iteration 600 : train acc = 0.79 , test acc = 0.7953\n",
      "iteration 1200 : train acc = 0.81 , test acc = 0.7853\n",
      "iteration 1800 : train acc = 0.83 , test acc = 0.7883\n",
      "iteration 2400 : train acc = 0.83 , test acc = 0.7753\n",
      "iteration 3000 : train acc = 0.85 , test acc = 0.787\n",
      "iteration 3600 : train acc = 0.83 , test acc = 0.7909\n",
      "iteration 4200 : train acc = 0.81 , test acc = 0.7869\n",
      "iteration 4800 : train acc = 0.82 , test acc = 0.783\n",
      "iteration 5400 : train acc = 0.71 , test acc = 0.7772\n",
      "iteration 6000 : train acc = 0.79 , test acc = 0.7679\n",
      "iteration 6600 : train acc = 0.83 , test acc = 0.7821\n",
      "iteration 7200 : train acc = 0.78 , test acc = 0.7793\n",
      "iteration 7800 : train acc = 0.8 , test acc = 0.7925\n",
      "iteration 8400 : train acc = 0.83 , test acc = 0.7796\n",
      "iteration 9000 : train acc = 0.88 , test acc = 0.7559\n",
      "iteration 9600 : train acc = 0.84 , test acc = 0.7884\n"
     ]
    }
   ],
   "source": [
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 데이터 획득 \n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = X_train[batch_mask]\n",
    "    t_batch = y_train[batch_mask]\n",
    "    \n",
    "    # 수치 기울기 계산\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    # 가중치 갱신 \n",
    "    params = network.params\n",
    "    optimizer.update(params, grads)\n",
    "    \n",
    "    # 학습 손실 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list[\"Adam\"].append(loss)\n",
    "    # print(\"iteration \", i+1, \": loss = \", loss) \n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_batch, t_batch)\n",
    "        test_acc  = network.accuracy(X_test,  y_test)\n",
    "        train_acc_list[\"Adam\"].append(train_acc)\n",
    "        test_acc_list[\"Adam\"].append(test_acc)\n",
    "        print(\"iteration\", i, \": train acc =\", train_acc, \", test acc =\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f05a04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 데이터 획득 \n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = X_train[batch_mask]\n",
    "    t_batch = y_train[batch_mask]\n",
    "    \n",
    "    # 수치 기울기 계산\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    # 가중치 갱신 \n",
    "    params = network.params\n",
    "    optimizer.update(params, grads)\n",
    "    \n",
    "    # 학습 손실 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list[\"SGD\"].append(loss)\n",
    "    # print(\"iteration \", i+1, \": loss = \", loss) \n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(X_train, t_batch)\n",
    "        test_acc  = network.accuracy(X_test,  y_test)\n",
    "        train_acc_list[\"SGD\"].append(train_acc)\n",
    "        test_acc_list[\"SGD\"].append(test_acc)\n",
    "        print(\"iteration\", i, \": train acc =\", train_acc, \", test acc =\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9a03ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4451007",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
