{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7129aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch1.4_p36/lib/python3.6/site-packages/ipykernel/__main__.py:1: UserWarning: Config option `use_jedi` not recognized by `IPCompleter`.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a8c2069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch1.4_p36/lib/python3.6/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.4.0 and strictly below 2.7.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.3.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('models.py') ) ))\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "import six\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from seqeval.metrics import classification_report as classification_report_seqeval\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from transformers import TFElectraModel, TFBertModel, AutoTokenizer\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Activation, Lambda, Input\n",
    "from tf2crf import CRF, ModelWithCRFLoss\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "from model.models import TFElectraClassifier\n",
    "import nltk\n",
    "import openpyxl\n",
    "import gc\n",
    "\n",
    "import random\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83ae7b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch1.4_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:1646: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('../vocab-NER.txt', do_lower_case=False)\n",
    "kos_tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00600dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EA01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EA02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EA03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EA04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EA05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>ND05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>ND06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>ND07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>ND08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>ND12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     code\n",
       "0    EA01\n",
       "1    EA02\n",
       "2    EA03\n",
       "3    EA04\n",
       "4    EA05\n",
       "..    ...\n",
       "140  ND05\n",
       "141  ND06\n",
       "142  ND07\n",
       "143  ND08\n",
       "144  ND12\n",
       "\n",
       "[145 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_file = pd.read_csv('../model/labels.txt', header=None, names=['code'], index_col=False)\n",
    "label_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85b17a63",
   "metadata": {
    "collapsed": true
   },
   
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2lab = {key: label_file['code'][key] for key in range(len(label_file))}\n",
    "idx2lab\n",
    "#label 144 개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f0a0e30",
   "metadata": {
    "collapsed": true
   },
   
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab2idx = {value:key for key,value in idx2lab.items()}\n",
    "lab2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7302f39",
   "metadata": {},
  
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/ubuntu/JIHO/NTIS/data/keywords_code_data.csv\",index_col=False)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e710549a",
   "metadata": {},
   
      "text/plain": [
       "   Unnamed: 0       keyword  code\n",
       "0           0      Hydrogen  EC01\n",
       "1           1  Purification  EC01\n",
       "2           2     Fuel cell  EC01\n",
       "3           3        Silica  EC01\n",
       "4           4    Separation  EC01"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cbb5d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop = df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "569ac32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1298573"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8831cc97",
   "metadata": {},
   
   "source": [
    "for k in tqdm(df_drop.iloc):\n",
    "    if type(k['keyword']) != str:\n",
    "        print(k['keyword'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44b89264",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1294248it [02:54, 7428.91it/s]\n"
     ]
    }
   ],
   "source": [
    "multi_keywords = []\n",
    "multi_codes = []\n",
    "for k in tqdm(df_drop.iloc):\n",
    "    if len(k['keyword'].split()) >= 2:\n",
    "        multi_keywords.append(k['keyword'])\n",
    "        multi_codes.append(k['code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d67c4b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568302 568302\n"
     ]
    }
   ],
   "source": [
    "print(len(multi_keywords),len(multi_codes))\n",
    "#멀티 키워드 , 라벨 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9e2a221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1298573it [02:51, 7553.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1298573"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = []\n",
    "codes = []\n",
    "for k in tqdm(df.iloc):\n",
    "    keywords.append(k['keyword'])\n",
    "    codes.append(k['code'])\n",
    "len(codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70e6e57",
   "metadata": {},
   "source": [
    "토크나이저 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "27f96ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['histone', 'acetylation']\n",
      "['Histone', 'acetylation']\n"
     ]
    }
   ],
   "source": [
    "print(kos_tokenizer.tokenize(multi_keywords[4]))\n",
    "print(tokenizer.tokenize(multi_keywords[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c889d5e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fuel', 'cell'], ['taste', 'sensation'], ['in', 'vivo', 'imaging'], ['multi', '##photon', 'microscopy'], ['histone', 'acetylation'], ['chromatin', 'loop', '##ing'], ['human', 'beta', 'glob', '##in', 'locus'], ['loop', 'mediating', 'protein'], ['hair', 'cell'], ['neural', 'circuit'], ['fusion', 'technology'], ['implant', '##able', 'ic'], ['bio', '-', 'information', 'processing'], ['stem', 'cells'], ['artificial', 'transcription', 'factor'], ['auditory', 'organ'], ['inner', 'ear'], ['middle', 'ear'], ['neural', 'crest'], ['mouse', 'model'], ['integrin', 'alpha', 'm'], ['innate', 'immune'], ['plant', 'disease'], ['fungal', 'pathogen'], ['rice', 'blast', 'disease'], ['plant', 'disease'], ['fungal', 'pathogen'], ['rice', 'blast', 'disease'], ['hepatitis', 'b', 'virus'], ['chronic', 'hepatitis', 'b'], ['drug', 'resistance'], ['antiviral', 'effect'], ['3', '-', 'dimensional', 'numerical', 'analysis'], ['local', 'sc', '##our'], ['river', 'management'], ['had', '##ley', 'circulation'], ['atmospheric', 'general', 'circulation'], ['natural', 'variability'], ['electro', '##kine', '##tic', 'energy', 'harvesting'], ['ionic', 'field', '-', 'effect', 'transistor'], ['graph', '##itic', 'carbon', 'materials'], ['surface', 'engineering'], ['surface', 'charge'], ['surface', 'slip'], ['cellular', 'damage', 'response'], ['regulatory', 'network'], ['development', 'of', 'targets'], ['genome', 'analysis'], ['physi', '##ome', 'analysis'], ['damage', 'repair', 'system'], ['chromosome', 'aberr', '##ation', '##analysis'], ['dna', 'methylation', 'assay'], ['reactive', 'oxygen', 'species'], ['3', '##d', 'nanop', '##rin', '##ting'], ['big', 'data'], ['decision', 'support', 'system'], ['collaborative', 'networks'], ['information', 'technology'], ['big', 'data'], ['decision', 'support', 'system'], ['collaborative', 'networks'], ['information', 'technology'], ['soft', 'matter'], ['complex', 'fluids'], ['none', '##quilibrium', 'statistical', 'mechanics'], ['meso', '##sc', '##opic', 'simulations'], ['lattice', 'boltzmann', 'method'], ['cell', 'membrane'], ['colloidal', 'dispersion', '##s'], ['electro', '##rhe', '##ological', 'fluids'], ['magnet', '##orh', '##e', '##ological', 'fluids'], ['stochastic', 'rotation', 'dynamics'], ['reinforced', 'concrete', 'beam'], ['externally', 'un', '##bond', '##ed'], ['ultim', '##ae', 'steel', 'stress'], ['section', 'analysis'], ['two', 'photon', 'microscopy'], ['two', 'photon', 'probes'], ['early', 'diagnosis', 'of', 'cancer'], ['single', 'nucleotide', 'polymorphism'], ['restriction', 'site', 'associated', 'dna', 'sequencing'], ['phylogenetic', 'relationships'], ['population', 'genetic', 'patterns'], ['carbon', 'storage'], ['carbon', 'budget', 'model'], ['climate', 'change'], ['natural', 'disturbance'], ['retin', '##itis', 'pigment', '##osa'], ['age', 'related', 'macular', 'degeneration'], ['induced', 'pluripotent', 'stem', 'cell'], ['tissue', 'eng', '##inner', '##ing'], ['cell', 'delivery', 'and', 'transplant', '##ion', 'system'], ['stem', 'cell', 'and', 'polymer', 'composite'], ['travel', 'demand', 'analysis'], ['social', 'network', 'theory'], ['social', 'spill', '-', 'over', 'effect'], ['activity', '-', 'based', 'travel', 'behavior', 'model'], ['agent', '-', 'based', 'model'], ['discrete', 'choice', 'model'], ['decision', '-', 'making', 'system']]\n",
      "[['Fuel', 'cell'], ['Ta', '##ste', 'sens', '##ation'], ['In', 'vivo', 'imaging'], ['Multi', '##ph', '##oton', 'microscopy'], ['Histone', 'acetylation'], ['Chromatin', 'loop', '##ing'], ['Human', 'beta', 'gl', '##obi', '##n', 'locus'], ['Loop', 'media', '##ting', 'protein'], ['hair', 'cell'], ['neural', 'circuit'], ['fusion', 'technology'], ['implant', '##able', 'IC'], ['bio', '[UNK]', 'information', 'processing'], ['stem', 'cells'], ['artificial', 'transcription', 'factor'], ['aud', '##itory', 'organ'], ['inner', 'ear'], ['midd', '##le', 'ear'], ['neural', 'cre', '##st'], ['mouse', 'model'], ['Integr', '##in', 'alpha', 'M'], ['In', '##nate', 'Immun', '##e'], ['Plant', 'Disease'], ['F', '##ung', '##al', 'Path', '##ogen'], ['Ric', '##e', 'bl', '##ast', 'disease'], ['Plant', 'Disease'], ['F', '##ung', '##al', 'Path', '##ogen'], ['Ric', '##e', 'bl', '##ast', 'disease'], ['Hep', '##atitis', 'B', 'virus'], ['Ch', '##ronic', 'Hep', '##atitis', 'B'], ['Drug', 'resistance'], ['Anti', '##viral', 'effect'], ['3', '[UNK]', 'Dimension', '##al', 'numerical', 'analysis'], ['local', 'sc', '##our'], ['r', '##iver', 'management'], ['Had', '##ley', 'Circ', '##ulation'], ['At', '##m', '##osph', '##eric', 'General', 'Circ', '##ulation'], ['Natural', 'Vari', '##ability'], ['Electro', '##kin', '##etic', 'energy', 'harvesting'], ['Ion', '##ic', 'field', '[UNK]', 'effect', 'transistor'], ['Graph', '##itic', 'carbon', 'materials'], ['Surface', 'engineering'], ['Surface', 'charge'], ['Surface', 'slip'], ['Cellular', 'damage', 'response'], ['Regulator', '##y', 'network'], ['Development', 'of', 'targets'], ['Genome', 'analysis'], ['Phys', '##io', '##me', 'analysis'], ['Damage', 'repair', 'system'], ['Chrom', '##osome', 'ab', '##er', '##ration', '##ana', '##ly', '##si', '##s'], ['DNA', 'methylation', 'assay'], ['Reactive', 'oxygen', 'species'], ['3', '##D', 'Nanop', '##rinting'], ['Big', 'data'], ['Decision', 'support', 'system'], ['Collabor', '##ative', 'networks'], ['Information', 'technology'], ['Big', 'data'], ['Decision', 'support', 'system'], ['Collabor', '##ative', 'networks'], ['Information', 'technology'], ['Soft', 'matter'], ['complex', 'fluid', '##s'], ['none', '##quilibrium', 'statistical', 'mechan', '##ics'], ['mes', '##oscopic', 'simulations'], ['lattice', 'Bolt', '##z', '##mann', 'method'], ['cell', 'membrane'], ['coll', '##oidal', 'dispersion', '##s'], ['electro', '##rh', '##e', '##ological', 'fluid', '##s'], ['magnet', '##or', '##he', '##ological', 'fluid', '##s'], ['Stochastic', 'Rot', '##ation', 'Dynamics'], ['Reinfor', '##ced', 'Concrete', 'Beam'], ['External', '##ly', 'Un', '##bon', '##ded'], ['Ul', '##tim', '##ae', 'Steel', 'Stress'], ['Section', 'Analysis'], ['two', 'photon', 'microscopy'], ['two', 'photon', 'probe', '##s'], ['early', 'diagnosis', 'of', 'cancer'], ['Single', 'nucleotide', 'polymorphism'], ['Rest', '##riction', 'site', 'associated', 'DNA', 'sequencing'], ['Ph', '##yl', '##ogenetic', 'relationship', '##s'], ['Pop', '##ulation', 'genetic', 'pattern', '##s'], ['Carbon', 'Storage'], ['Carbon', 'Bu', '##dge', '##t', 'Model'], ['Cl', '##imate', 'Change'], ['Natural', 'Dis', '##turb', '##ance'], ['retin', '##itis', 'pi', '##g', '##ment', '##osa'], ['ag', '##e', 'related', 'mac', '##ular', 'deg', '##eneration'], ['Ind', '##uced', 'pluripot', '##ent', 'stem', 'cell'], ['tissue', 'en', '##gin', '##ner', '##ing'], ['cell', 'delivery', 'and', 'trans', '##plant', '##ion', 'system'], ['stem', 'cell', 'and', 'polymer', 'composite'], ['Tra', '##vel', 'Dem', '##and', 'Analysis'], ['Social', 'Network', 'Theory'], ['Social', 'Sp', '##ill', '[UNK]', 'over', 'Effect'], ['Activity', '[UNK]', 'based', 'Tra', '##vel', 'Behavior', 'Model'], ['Agent', '[UNK]', 'based', 'Model'], ['Disc', '##rete', 'Ch', '##oice', 'Model'], ['Decision', '[UNK]', 'Ma', '##king', 'System']]\n"
     ]
    }
   ],
   "source": [
    "sci_tokenized_multi = []\n",
    "tokenized_multi = []\n",
    "for k in multi_keywords[:100]:\n",
    "    sci_tokenized_multi.append(kos_tokenizer.tokenize(k))\n",
    "    tokenized_multi.append(tokenizer.tokenize(k))\n",
    "print(sci_tokenized_multi)\n",
    "print(tokenized_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2551d55e",
   "metadata": {},
   "source": [
    "label idx 로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab802329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568302"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code2idx = []\n",
    "for k in multi_codes:\n",
    "    code2idx.append(lab2idx[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fa26f72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568302 568302\n"
     ]
    }
   ],
   "source": [
    "print(len(multi_keywords),len(code2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab0a408",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "acebfa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "825baa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/568302 [00:00<?, ?it/s]/home/ubuntu/anaconda3/envs/pytorch1.4_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "100%|██████████| 568302/568302 [02:04<00:00, 4561.66it/s]\n"
     ]
    }
   ],
   "source": [
    "encode_train_data = []\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "token_type_ids =[]\n",
    "for line in tqdm(multi_keywords):\n",
    "    encoded_dict = tokenizer.encode_plus(line,\\\n",
    "                                        add_special_tokens=True,\\\n",
    "                                        pad_to_max_length=True,\\\n",
    "                                        max_length = MAX_LENGTH,\\\n",
    "                                        return_attention_mask=True,\\\n",
    "                                        truncation =True)\n",
    "    \n",
    "    input_id = encoded_dict['input_ids']\n",
    "    attention_mask=encoded_dict['attention_mask']\n",
    "    token_type_id = encoded_dict['token_type_ids']\n",
    "    \n",
    "    input_ids.append(input_id)\n",
    "    attention_masks.append(attention_mask)\n",
    "    token_type_ids.append(token_type_id)\n",
    "\n",
    "train_input_ids=np.array(input_ids, dtype=int)\n",
    "train_attention_masks=np.array(attention_masks, dtype=int)\n",
    "train_token_type_ids=np.array(token_type_ids, dtype=int)\n",
    "train_x=(train_input_ids, train_attention_masks, train_token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6088610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eb8022",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch1.4_p36)",
   "language": "python",
   "name": "conda_pytorch1.4_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
